# ðŸš€ Accelera Project Flow Analysis

## ðŸ“‹ Table of Contents
1. [Execution Flow Overview](#execution-flow-overview)
2. [CPU vs GPU Starting Point](#cpu-vs-gpu-starting-point)
3. [Fallback Mechanisms](#fallback-mechanisms)
4. [Chunking Strategies](#chunking-strategies)
5. [Memory Management Flow](#memory-management-flow)

---

## ðŸ”„ Execution Flow Overview

### 1. **Entry Point: PyTorch Interception**
```
User Code: torch.matmul(a, b)
    â†“
accelera/interceptor.py â†’ accelera_matmul()
    â†“
Check: should_intercept() â†’ Size & Memory Thresholds
    â†“
Route to: MemoryEfficientEngine
```

### 2. **Engine Decision Tree**
```
MemoryEfficientEngine.matmul()
    â†“
Calculate Memory Requirements
    â†“
Check: _should_use_fallback() â†’ threshold < 0.05GB
    â†“
Branch A: CPU Fallback     Branch B: Default Chunk     Branch C: Normal Processing
```

### 3. **Result Processing**
```
Matrix Result â†’ Convert to Tensor â†’ Return to User
```

---

## ðŸ’» CPU vs GPU Starting Point

### **ðŸ Starting Point: ALWAYS CPU**
The Matrix class **defaults to CPU storage**:

```python
# In matrix.py __init__()
self._storage_device = torch.device(device if device is not None else 'cpu')
self._tensor = self._tensor.to(self._storage_device)
```

**Key Characteristics:**
- âœ… **Data starts on CPU** for memory safety
- âœ… **GPU transfers happen during computation** only
- âœ… **Results return to original device** (CPU or GPU as specified)
- âœ… **Temporary GPU usage** with aggressive cleanup

### **ðŸ”„ CPU â†” GPU Transfer Flow**
```
Input Data (CPU) 
    â†“ 
Accelera Processing (GPU/CPU based on strategy)
    â†“
Result (Back to original device)
```

---

## ðŸ›¡ï¸ Fallback Mechanisms

### **ðŸ“Š Fallback Criteria**

| Criteria | Threshold | Action |
|----------|-----------|--------|
| **Memory Threshold** | `< 0.05GB` (50MB) | Trigger Fallback |
| **GPU OOM** | Runtime Error | CPU Computation |
| **Insufficient GPU Memory** | `< 0.3GB available` | CPU Chunks |

### **ðŸŽ¯ Fallback Decision Points**

#### **1. Primary Fallback Check**
```python
def _should_use_fallback(self, total_memory_gb: float) -> bool:
    return self.memory_threshold_gb < 0.05  # Less than 50MB threshold
```

#### **2. Strategy Selection**
```python
if self._should_use_fallback(total_memory_gb):
    if self.fallback_strategy == "cpu":
        # CPU Fallback Strategy
    elif self.fallback_strategy == "default_chunk":
        # Default Chunking Strategy
```

### **ðŸ“ Fallback Locations**

#### **Location 1: MemoryEfficientEngine.matmul()**
- **File**: `accelera/memory_efficient_engine.py`
- **Line**: ~130
- **Trigger**: Very low memory threshold (`< 0.05GB`)

#### **Location 2: Engine Chunking Logic**
- **File**: `accelera/engine.py` 
- **Lines**: ~210, ~235, ~250, ~265
- **Trigger**: GPU OOM during chunk processing

#### **Location 3: Memory Manager**
- **File**: `accelera/engine.py`
- **Line**: ~150
- **Trigger**: Insufficient memory for direct computation

---

## ðŸ”§ Chunking Strategies

### **ðŸ“‹ Available Chunking Strategies**

#### **1. Row Chunking** (`RowChunking`)
- **Description**: Processes fixed number of rows at a time
- **Use Case**: Most common, memory-efficient
- **Default Chunk Size**: Adaptive based on memory
- **File**: `accelera/chunking.py`

```python
def get_chunks(self, shape, chunk_size):
    for start in range(0, rows, chunk_size):
        end = min(start + chunk_size, rows)
        chunk_slice = (slice(start, end),) + ...
```

#### **2. Tile Chunking** (`TileChunking`)
- **Description**: 2D tiles for better cache locality
- **Use Case**: Very large matrices needing 2D chunking
- **Configuration**: `tile_size: (rows, cols)`
- **File**: `accelera/chunking.py`

#### **3. Adaptive Chunking** (`AdaptiveChunking`)
- **Description**: Adjusts chunk size based on memory pressure
- **Use Case**: Default strategy, optimal performance
- **Features**: Memory monitoring, dynamic adjustment
- **File**: `accelera/chunking.py`

### **ðŸŽ¯ Default Chunking Strategy**

#### **Strategy Used**: **Adaptive Chunking**
```python
def _create_chunking_strategy(self, strategy_type: str):
    if strategy_type == 'adaptive':  # DEFAULT
        return AdaptiveChunking(self.memory_manager)
```

#### **Default Chunk Sizes**:
- **Primary**: `1024` (MatrixEngine default)
- **Fallback**: `1000` (MemoryEfficientEngine default chunking)
- **Minimum**: `1` (AdaptiveChunking minimum)

#### **Default Chunking in MemoryEfficientEngine**:
```python
def _use_default_chunking(self, a, b):
    default_chunk_size = 1000  # Process 1000 rows at a time
    # Simple row-based chunking regardless of memory constraints
```

**Characteristics**:
- âœ… **Fixed 1000-row chunks**
- âœ… **GPU processing** (not CPU like the CPU fallback)
- âœ… **Memory-agnostic** (ignores memory constraints)
- âœ… **Simpler than adaptive chunking**

---

## ðŸ§  Memory Management Flow

### **ðŸ” Memory Decision Tree**

```
1. Calculate Operation Memory Requirements
    â†“
2. Check Memory Threshold (< 0.05GB?)
    â†“ YES                          â†“ NO
3A. Fallback Strategy          3B. Normal Processing
    â†“                              â†“
4A. CPU or Default Chunk       4B. Direct GPU or Chunked
    â†“                              â†“
5A. CPU Computation            5B. GPU Computation
   or Simple GPU Chunks           with Memory Management
```

### **ðŸ“Š Memory Calculation**
```python
# In MemoryEfficientEngine.matmul()
bytes_per_element = 4  # float32
a_memory_gb = (a.numel() * bytes_per_element) / (1024**3)
b_memory_gb = (b.numel() * bytes_per_element) / (1024**3)  
result_memory_gb = (a.shape[0] * b.shape[1] * bytes_per_element) / (1024**3)
total_memory_gb = a_memory_gb + b_memory_gb + result_memory_gb
```

### **ðŸŽ¯ Key Decision Points**

| Memory Threshold | Action | Engine | Location |
|------------------|--------|---------|----------|
| `< 0.05GB` | CPU Fallback/Default Chunk | MemoryEfficientEngine | Line ~130 |
| `>= threshold` | Direct GPU | MatrixEngine | Line ~140 |
| `Insufficient GPU` | Chunked GPU | MatrixEngine | Line ~150 |
| `< 0.3GB available` | CPU Chunks | MatrixEngine | Line ~210 |

---

## ðŸŽ‰ Summary

### **ðŸ—ï¸ Architecture Highlights**
1. **CPU-First Design**: All data starts on CPU for safety
2. **Dual Fallback System**: CPU computation + Default chunking
3. **Adaptive Memory Management**: Dynamic chunk sizing
4. **Transparent Operation**: Users don't change code
5. **Memory Leak Prevention**: Aggressive cleanup at all levels

### **ðŸ”„ Flow Summary**
```
PyTorch Call â†’ Interception â†’ Memory Analysis â†’ Strategy Selection â†’ Computation â†’ Cleanup â†’ Return
```

The system is designed to **gracefully degrade** from optimal GPU processing to memory-efficient alternatives while maintaining **mathematical correctness** and **preventing memory leaks**.